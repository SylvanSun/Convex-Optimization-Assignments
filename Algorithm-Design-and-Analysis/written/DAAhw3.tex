\documentclass[12pt,letterpaper]{article}
\usepackage{fullpage}
\usepackage[top=2cm, bottom=4.5cm, left=2.5cm, right=2.5cm]{geometry}
\usepackage{amsmath,amsthm,amsfonts,amssymb,amscd}
\usepackage{lastpage}
\usepackage{enumerate}
\usepackage{fancyhdr}
\usepackage{mathrsfs}
\usepackage{xcolor}
\usepackage{graphicx}
\usepackage{listings}
\usepackage{hyperref}
\usepackage{mathtools}
\usepackage{xfrac}
\usepackage{algorithm}
\usepackage[noend]{algpseudocode}

\hypersetup{%
  colorlinks=true,
  linkcolor=blue,
  linkbordercolor={0 0 1}
}
\linespread{1.1}
 
\renewcommand\lstlistingname{Algorithm}
\renewcommand\lstlistlistingname{Algorithms}
\def\lstlistingautorefname{Alg.}

\lstdefinestyle{Python}{
    language        = Python,
    frame           = lines, 
    basicstyle      = \footnotesize,
    keywordstyle    = \color{blue},
    stringstyle     = \color{green},
    commentstyle    = \color{red}\ttfamily
}

\setlength{\parindent}{0.0in}
\setlength{\parskip}{0.05in}

% Edit these as appropriate
\newcommand\course{Algorithms}
\newcommand\hwnumber{3}                
\newcommand\NetIDa{SUN Yilin}          
\newcommand\NetIDb{520030910361}          

\pagestyle{fancyplain}
\headheight 35pt
\lhead{\NetIDa}
\lhead{\NetIDa\\\NetIDb}           
\chead{\textbf{\Large Homework \hwnumber}}
\rhead{\course \\ \today}
\lfoot{}
\cfoot{}
\rfoot{\small\thepage}
\headsep 1.5em

\begin{document}

\section{}
\textbf{Algorithm:}\\
We sort the customers by ascending order of their service time,
then serve the customer with smallest service time each time.\\
\textbf{Proof of Correctness:}\\
The total time can be expressed as 
$$T=\sum_{i=1}^{n}\sum_{j=1}^{i}t_j$$
which is simply
$$T=\sum_{i=1}^{n}(n-i+1)t_i$$
So if the sequence $\{t_1,t_2,t_3,\dots,t_n\}$ is in ascending order, i.e.
$t_1\leq t_2\leq t_3\leq\dots\leq t_n$,
simply by the \textbf{rearrangement inequality} we know such a $T$ is the minimum waiting time
because it is the so-called "inverse product sum".\\
\newline
Below is the proof of rearrangement inequality for completeness of this problem.\\
Denote the two sequence that have already sorted by ascending order by $\{a_i\}$ and $\{b_i\}$.
Let $p_i=\sum_{j=1}^{i}b_j$, $q_i=\sum_{j=1}^{i}b_{k_j}$. It's easy to see $p_i\leq q_i$ for $i\leq n-1$ and $p_n=q_n$.\\
Then 
\begin{align}
    \sum_{i=1}^{n}a_{n-i+1}b_{i}&=\sum_{i=1}^{n-1}a_{i}(p_{n-i+1}-p_{n-i})+a_np_1\\
    &=\sum_{i=1}^{n-1}(a_{n-i+1}-a_{n-i})p_i+a_1p_n\\
    &\leq \sum_{i=1}^{n-1}(a_{n-i+1}-a_{n-i})q_i+a_1q_n\\
    &=\sum_{i=1}^{n}a_{n-i+1}b_{k_i}
\end{align}
Another side of the inequality is not used in our problem, 
so the proof is omitted here.

\newpage
\section{}
\subsection{}
Prove by contradiction. Suppose there are two maximal independent sets denoted by $A$ and $A'$ with different size,
without loss of generality, assume $|A|<|A'|$.
Then by exchange property $\exists x\in A'\backslash A$ such that $A\cup\{x\}\in\mathcal{I}$.
This means that there exists $A\cup\{x\}\in\mathcal{I}$ and $A\subset A\cup\{x\}$,
which contradicts with the fact that $A$ is a maximal independent set.
So maximal independent sets must have the same size.
\subsection{}
We first prove the hereditary property.\\
Obviously $\mathcal{S}$ is non-empty because at least it contains the empty set.
For any $P\in\mathcal{S}$ and any $Q\subset P$, since $P$ is acyclic, $Q$ cannot be acyclic,
so $Q\in\mathcal{S}$. So $M$ has the hereditary property.\\
\newline
We then prove the exchange property.\\
For any two independent sets $A,B\in\mathcal{S}$, without loss of generality, assume $|A|<|B|$.
We consider the forest $G'$ induced by $A$ in $G$.
Suppose there are $k$ trees in $G'$ and the number of edges of each tree is denoted by $a_i$
then we have 
$$\sum_{i=1}^ka_i=|A|$$
Now we consider the "distribution" of edges in $B$.
Since $B\in\mathcal{S}$, $B$ is acyclic.
Then the maximum number of edges from $B$ connecting vertices in each tree of $G'$ is at most $a_i,i\in[k]$.
But we know $|B|>|A|=\sum_{i=1}^ka_i$, 
which means that there is at least one edge in $B$ connecting one(or two) vertex out of $G'$ or
connecting two different trees in $G'$, and we denote this specific edge by $e$. 
Then we know $A\cup\{e\}$ is not acyclic. 
And since it is acyclic, it is in $\mathcal{S}$. 
So there exists $e\in B\backslash A$ such that $A\cup\{e\}\in\mathcal{S}$.
So $M$ has the exchange property.\\
To sum up, $M$ is a matroid.\\
The maximal sets of this matroid are all the spanning trees of graph $G$.

\subsection{}
Prove by contradiction. Suppose there is no maximal independent set with maximum weight containing $x$,
pick an arbitrary maximal set with maximum weight denoted by $S$ and we will show that we can construct another maximal set $S'$ with maximum weight from $S$.\\
We will start from $S$ and $\{x\}$. Our condition here is that $x\notin S$ and $x$ has the maximum weight.
By exchange property $\exists x_{i_1}\in S$ such that $\{x,x_{i_1}\}\in\mathcal{I}$.
Moreover, we can carry out this process as long as $|S|>|\{x,x_{i_1}\}|$.
By exchange property we carry out this process $|S|-1$ times and get a set 
$$S'=\{x,x_{i_1},x_{i_2},x_{i_3},\dots,x_{i_{|S|-1}}\}$$
$S'$ satisfies that $S'\backslash\{x\}\subset S$, $|S'|=|S|$ and $S'\in\mathcal{I}$.\\
The fact that $|S'|=|S|$ and $S'\in\mathcal{I}$ tells us that $S'$ is a maximal independent set, 
and the fact that $S'\backslash\{x\}\subset S$, $x$ has maximum weight tells us that $w(S')\geq w(S)$.
Since $S$ has maximum weight, $S'$ also has maximum weight. 
So we have constructed a maximal independent set with maximum weight containing $x$,
which is $S'$.

\subsection{}
To use this algorithm we only need to formalize our input.\\
For the MST problem we have a graph $G=(V,E)$ and the matroid is properly defined and proved as shown in problem $2.2$,
which is $M=(E,\mathcal{S})$. 
The weight function $w$ is also defined on $E\to \mathbb{R}_{\geq 0}$ in the MST problem.
We then use $M$ and $w$ as inputs for our algorithm.\\
Then this algorithm will pick the element in $E$ with the largest weight as long as $S\cup \{x\}\in\mathcal{I}$,
which is equivalent to picking an edge with largest weight as long as the edges does not generate a cycle in Kruskal's algorithm.
So after we run this algorithm, we will get a maximal(or minimal) spanning tree.

\subsection{}
We define $\mathcal{S}=\{F\subset U|$vectors in $F$ are linearly independent$\}$.
We now prove $M=\{U,\mathcal{S}\}$ is a matroid and use the algorithm described in problem 2.3 with $M$ and $w$ as input.\\
Firstly, the hereditary property is true because any subset of a set containing vectors that are linearly independent must also be containing independent vectors.\\
As for the  exchange property, we prove by constadiction.\\
For any two independent sets $A,B\in\mathcal{S}$, without loss of generality, assume $|A|<|B|$.
If exchange property does not hold for $A$ and $B$, then since $A$ is linearly independent,
$B$ can be linearly expressed by $A$.
Since $B$ can be linearly expressed by a set of vertors with size smaller than $|B|$,
$B$ itself cannot be linearly independent.
So there exists a vector $x\in B$ where $x\notin A$ and $A\cup\{x\}\in\mathcal{S}$.\\
So $M$ is indeed a matroid. Then as we have stated at the start of this problem,
we can run algorithm in 2.3 with $M$ and $w$ and get such a set of linearly independent vectors $S$ with maximum weight.

\newpage
\section{}
\subsection{}
The idea is pretty straightforward.
Since we only need to determine whether B is reachable from A, 
the gas cost is not to our concern here.
So starting from $A$, we fill up the gas tank as soon as we arrive at a gas station.
Following this method, as long as the two adjacent gas stations are within the distance of our tank capacity $C$,
we can always reach the next station from our current station. \\
Given the inputs, i.e. the distance of these $n$ gas stations, we put them in an array $\boldsymbol{d}$ and sort it by ascending order.
Then we calculate the diffenence between adjacent elements in array $\boldsymbol{d}$.
If any of the diffenences is larger than $C$, 
or if the distance between farthest gas station and $D$ is larger than $C$, then $B$ is not reachable.
If not, then $B$ is reachable.  
\begin{algorithm}
  \caption{Determine Reachable}\label{alg:deterreach}
  \begin{algorithmic}[1]
  \Procedure{Determine Reachable}{$\boldsymbol{d},C,D$}:\Comment{$\boldsymbol{d}=\{d_1,d_2,d_3,\dots,d_n\}$}
  \State Sort $\boldsymbol{d}$ by ascending order
  \If{$D-d_n>C$}
  \State \textbf{return} Not Reachable
  \EndIf
  \State Initialize $i\gets 0$
  \While{$i<n$}
  \State $l\gets d_{i+1}-d_i$
  \If{$l>C$}
  \State \textbf{return} Not Reachable
  \EndIf
  \EndWhile
  \State \textbf{return} Reachable
  \EndProcedure
  \end{algorithmic}
\end{algorithm}\\
\textbf{Proof of Correctness:}\\
The algorithm is correct because, as is described above, 
it always guarantees that the reachability of next station can be determined by the nearest gas station before it.
Thus if all the gas stations are reachable from the previous stations and $D$ is reachable from the farthest station, 
$B$ is reachable from $A$.\\
\textbf{Analysis of Running Time:}\\
The time complexity of the sort procedure at line 2 is $O(n\log n)$.
After we sort the sequence, the while loop will finish with time $O(n)$.
So the total running time is $O(n\log n)$.\\
If the input in $\boldsymbol{d}$ is given us ordered,
the total running time is just $O(n)$.

\newpage
\subsection{}
Since the total distance $D$ is definite,
the total amount of gas needed is $D$.
To be specific, we do not have to pre-fuel the tank to reach $B$.\\
So the idea is to always use the cheapest gas possible.
We put the price of $n$ gas stations and put them in an array $\boldsymbol{p}$,
then we sort $\boldsymbol{p}$ in a ascending order. 
We pick the cheapest $p_i$ each time and calculate the interval
on which we can use the gas from $i$-th station to travel.
A detailed algorithm is below:
\begin{algorithm}
  \caption{Minimal Cost}\label{alg:mcost}
  \begin{algorithmic}[1]
  \Procedure{Minimal Cost}{$\boldsymbol{p},\boldsymbol{d},C,D$}:
  \State Make pair of elements in $\boldsymbol{p}$ and $\boldsymbol{d}$ for sort
  \State Sort $\boldsymbol{p}$ by ascending order of $p_i$
  \State Initialize an empty interval $L=\emptyset$\Comment{Update $L$ at each iteration}
  \State Initialize $c\gets 0$\Comment{$c$ is the total cost}
  \State Initialize $i\gets 0$
  \While{$i\leq n$}
  \State calculate a closed interval $L_i=[d_i,d_i+C]\cap[0,D]$
  \State calculate the length $l_i$ of $L_i\backslash L$\Comment{O($\log n$) with binary search tree}
  \State $c\gets c+p_il_i$\Comment{Update the total cost}
  \State $L\gets L\cup L_i$\Comment{O($\log n$) by maintaining a binary search tree}
  \If{$L=[0,D]$}
  \State \textbf{return} $c$
  \EndIf
  \EndWhile
  \State \textbf{return} -1
  \EndProcedure
  \end{algorithmic}
\end{algorithm}\\
Actually this algorithm can also help us determine whether $B$ is reachable.
If the result given by this algorithm is negative,
then $B$ is not reachable.\\
\textbf{Proof of Correctness:}\\
Suppose there is another gas expenditure plan with minimal total cost,
we will show that our algorithm will provide a solution at least better.\\
We can assume that our car can sort of "remember" to use gas
from which gas station on any of these intervals.
Or to be specific, on the interval $Li\backslash L$ of each iteration,
we assume our car is using gas from the $i$-th gas station.
Under this definition, 
we prove our "gas plan" is at least better than another optimal gas plan by induction.\\
In this induction, we construct our plan $P_n$ from $P^*$ and show that 
$P_i$ is a improvement over $P^*$ at each iteration.\\
Let $P_0=P^*$ and we then do induction.\\
Outside interval $L_1\backslash L$, we use plan $P_0$,
on interval $L_1\backslash L$, we use our plan $P_n$, 
we update $P_1$ by combining the plans at $P_0$ and $P_n$ on the whole interval $[0,D]$.
then obviously $P_1$ is better than $P_0=P^*$ because at least it is better on $L_1\backslash L$.\\
We now construct $P_{k}$ from $P_{k-1}$:\\
Outside interval $L_k\backslash L$, we use plan $P_{k-1}$,
on interval $L_k\backslash L$, we use our plan $P_n$, 
Then at each iteration our $P_k$ is at least an improvement on interval $L_k\backslash L$
while the cost outside $L_k\backslash L$ remain unchanged.
So after the iterations finally our $P_n$ is better than $P_0=P^*$.
Since $P^*$ is optimal, our $P_n$ is also optimal.\\
\textbf{Analysis of Running Time:}\\
aaa

\newpage
\section{}
\subsection{}
The idea is to initialize a root $r$ with degree larger or equal to 2,
make the parent of $r$ itself,
then do BFS from $r$ and calculate the depth of all the vertices.
After this find the deepest leaf and find its parent $p$.\\
Then do BFS from $p$ for one step to mark all the vertices that is connected to $p$ as covered.
At each iteration, we pick a single parent $p$ and mark some vertices.\\
Then we find the deepest leaf that has not been covered and repeat the process
until all the vertices are covered in the graph.\\
\begin{algorithm}
  \caption{1-Minimal Cover}\label{alg:1cover}
  \begin{algorithmic}[1]
  \Procedure{1-Minimal Cover}{$G=(V,E)$}:\Comment{G must be a tree}
  \State Initialize set of vertices $S$, $S$ is empty
  \State Initialize an arbitrary root $r$ of $G$, let parent$(r)=r$\Comment{deg($r$)$\geq$2 for convenience} 
  \State BFS($r$) to get the depth of all vertices in $V$
  \While {$V$ is not totally covered}
  \State $p\gets$ Find parent of the \textbf{deepest uncovered} leaf in $G$\Comment{O(1) by "depth" vector}
  \State $S\gets S\cup\{p\}$
  \State Mark all vertices connected to $p$ as already covered.\Comment{BFS for one step}
  \EndWhile
  \State \textbf{return} $S$
  \EndProcedure
  \end{algorithmic}
\end{algorithm}\\
\textbf{Proof of Correctness:}\\
We prove by showing that our choice of vertices is necessary at each iteration 
and is global optimal as well.\\
For convenience of analysis, suppose at current iteration $|V|>2$, 
we know that an optimal minimal cover set must cover all the leaves in $G$,
and we also know that the deepest uncovered leaf can ony be covered
by picking itself or its parent.
But obviously the choice of parent is better than picking the leaf itself
because the parent can cover at least the same set of vertices as its child.
So our choice of parents is indeed necessary at each iteration.\\
The reason why the update process at Line 8
does not violate global optimal is that
once a vertex is covered, 
we do not have to consider it ever again.
So again we find the deepest uncovered leaf and carry out the same procedure.
The reason why we choose the deepest leaf is not explained here because
for $k=1$ it is not that obvious why we must choose the deepest one each iteration.
But we will show that for general $k$ this algorithm will not perform well
if not choosing the deepest leaf.\\
To sum up, our choice of vertices at each iteration must be necessary,
or equivalently and more specificly, for any optimal cover set for this tree,
at least all the vertices chosen by our algorithm must be in this optimal set 
so it can generate the minimal cover set of $G$.\\
\textbf{Analysis of Running Time:}\\
Since $|E|=|V|-1$, we can describe the time complexity only by $|V|$.
The time spent by our algorithm that is not constant time only comes from
BFS at line 4 and while loop at line 5.
The BFS at line 4 will traverse the whole tree once
and the while loop will only mark all the vertices once.
As for the BFS at line 8,
it will be carried out within constant time because we only do BFS for one step.
So the algorithm kind of traverse the graph twice,
causing a time complexity of $O(|V|)$.

\newpage
\subsection{}
The idea is similar to the case with $k=1$ 
and we only need to make some subtle changes to our algorithm.
But pay attention that this time the choice of the deepest uncovered leaf 
at each iteration is crucial to the correctness of our algorithm.\\
The first change is that we do not pick the parent of a leaf.
We still need to find all the deepest leaf,
but we are going to pick the ancestor $p$
which has a path with exactly length $k$ of the leaf.
Then we do BFS from $p$ for $k$ steps and mark all the vertices that is
within distance $k$ with $p$ as covered.
Similarly at each iteration we find an ancestor and mark some vertices.\\
The other change is how we exit the procedure.
We cannot guarantee that after some iterations the deepest uncovered leaf 
still has an ancestor with exactly distance $k$
So we pick the root when the depth of deepest uncovered leaf  
is smaller or equal to $k$ and end the procedure.\\
\begin{algorithm}
  \caption{k-Minimal Cover}\label{alg:kcover}
  \begin{algorithmic}[1]
  \Procedure{k-Minimal Cover}{$G=(V,E)$}:\Comment{G must be a tree}
  \State Initialize set of vertices $S$, $S$ is empty
  \State Initialize an arbitrary root $r$ of $G$\Comment{deg($r$)$\geq$2 for convenience}
  \State BFS($r$) to get the depth of all vertices in V
  \While {$V$ is not totally covered}
  \State $l\gets$ the deepest uncovered leaf
  \If {the depth of $l$ is smaller or equal to $k$}
  \State $S\gets S\cup\{r\}$
  \State \textbf{return} $S$
  \EndIf
  \State $p\gets$ Find ancestor with exactly distance $k$ of $l$
  \State $S\gets S\cup \{p\}$
  \State Mark all vertices within distance $k$ of $p$ as already covered 
  \EndWhile
  \State \textbf{return} $S$
  \EndProcedure
  \end{algorithmic}
\end{algorithm}\\
\textbf{Proof of Correctness:}\\
Similar to our proof with $k=1$, at each itertaion,
our choice of the ancestor $p$ is necessary because 
we still need to cover the current deepest leaf.
And to cover this leaf,
using the ancestor $p$ with exactly distance $k$ is the best choice 
because $p$ is capable of covering any other vertices
that can be covered by $p'$s descendants.
But we cannot choose any ancestor of $p$
because that will cause current deepest leaf remaining uncovered.\\
The reason why we choose the deepest uncovered leaf
is revealed here:
Only by choosing the deepest one can we guarantee that
this leaf cannot be covered by other leaves' ancestors chosen in this way.\\
\textbf{Analysis of Running Time:}\\
Although this problem is a general one, 
it turns out that the total running time is the same as 
the case where $k=1$
because we still only need to traverse each single vertex constant times.
So the total time complexity is still $O(|V|)$.

\section{Comments}
\subsection{}
About two whole days excluding time spent on sleeping and eating(and loafing perhaps).....
\subsection{}
Actually the most difficult question from my perspective is problem 2 about matroid.
I do not think I will be able to work out this problem without the hint.
The hint actually reduces the time spent on thinking largely.
But I will still give this problem a big $5$.\\
Problem $4$ will also get a $5$. 
Although the intuition of this problem is not that hard to find,
the implement for a formal algorithm surely takes some time.
Even if you finish your algorithm, 
you might still need to modify it several times.
Moreover, the running time analysis also takes a lot of work.\\
As for problem $3$ the algorithm is not that hard to figure out,
what takes time is how to prove the correctness and analyze the running time.
So my recommendation is $4$.\\
Problem 1 is down-right application of rearrangement inequality, so the rating is $2$.
If the proof of rearrangement inequality is not required, the rating can be reduced to $1$.
\subsection{}
No collaborators this time.(Big thanks to the hint.)
\end{document}
