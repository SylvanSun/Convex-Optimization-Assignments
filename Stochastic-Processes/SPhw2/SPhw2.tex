\documentclass[12pt,letterpaper]{article}
\usepackage{fullpage}
\usepackage[top=2cm, bottom=4.5cm, left=2.5cm, right=2.5cm]{geometry}
\usepackage{amsmath,amsthm,amsfonts,amssymb,amscd}
\usepackage{lastpage}
\usepackage{enumerate}
\usepackage{fancyhdr}
\usepackage{mathrsfs}
\usepackage{xcolor}
\usepackage{graphicx}
\usepackage{listings}
\usepackage{hyperref}
\usepackage{mathtools}
\usepackage{xfrac}


\hypersetup{%
  colorlinks=true,
  linkcolor=blue,
  linkbordercolor={0 0 1}
}
\linespread{1.1}
 
\renewcommand\lstlistingname{Algorithm}
\renewcommand\lstlistlistingname{Algorithms}
\def\lstlistingautorefname{Alg.}


\lstdefinestyle{Python}{
    language        = Python,
    frame           = lines, 
    basicstyle      = \footnotesize,
    keywordstyle    = \color{blue},
    stringstyle     = \color{green},
    commentstyle    = \color{red}\ttfamily
}

\setlength{\parindent}{0.0in}
\setlength{\parskip}{0.05in}

% Edit these as appropriate
\newcommand\course{Stochastic Processes}
\newcommand\hwnumber{2}                  % <-- homework number
\newcommand\NetIDa{SUN Yilin}           % <-- NetID of person #1
\newcommand\NetIDb{520030910361}           % <-- NetID of person #2 (Comment this line out for problem sets)

\pagestyle{fancyplain}
\headheight 35pt
\lhead{\NetIDa}
\lhead{\NetIDa\\\NetIDb}                 % <-- Comment this line out for problem sets (make sure you are person #1)
\chead{\textbf{\Large Homework \hwnumber}}
\rhead{\course \\ \today}
\lfoot{}
\cfoot{}
\rfoot{\small\thepage}
\headsep 1.5em

\begin{document}

\section{Optimal Coupling}
\subsection{Basic Ideas}
To reach the lower bound $D_{TV}(\mu,\nu)$, the intuition is that we maximize the terms where $X=Y$. Denote our optimal coupling by $\omega^*$, then $\forall (x,y)\in\Omega^2$ such that $x=y$, the maximum of $\omega^*(x,y)$ can only be $\min\{\mu(x),\nu(y)\}$. As we want to maximize these cases, we can directly set it as $\min\{\mu(x),\nu(y)\}$.\\
Then we need a formula to define $\omega^*(x,y)$ where $x\neq y$. Here the intuition is that $\omega^*(x,y)$ should be some constant times $\max\{\mu(x)-\nu(x),0\}\cdot\max\{\nu(y)-\mu(y),0\}$. So up to here, I construct a coupling as follows:\\
$$\omega^*(x,y)=\begin{cases}
\min\{\mu(x),\nu(y)\},\quad x=y\\
C\cdot\max\{\mu(x)-\nu(x),0\}\cdot\max\{\nu(y)-\mu(y),0\},\quad o.w.
\end{cases}$$
Now I will determine $C$ so that $\omega^*$ is indeed a valid coupling, and then show $Pr_{(X,Y)\sim\omega^*}(X\neq Y)$ is actually $D_{TV}(\mu,\nu)$.
\subsection{Determine $C$ and proof of valid coupling}
Define $A=\{x\in\Omega|\mu(x)\geq\nu(x)\}$. It follows that $\bar{A}=\{x\in\Omega|\mu(x)<\nu(x)\}$. We now calculate the marginal distribution of $X$ under $\omega^*$.\\
\begin{align}
  \forall x\in A,\quad \sum_{y\in\Omega}\omega^*(x,y)&=\sum_{y\in\Omega\wedge y=x}\omega^*(x,y)+\sum_{y\in\Omega\wedge y\neq x}\omega^*(x,y)\\
  &=\min\{\mu(x),\nu(x)\}+\sum_{y\in\Omega\wedge y\neq x}\omega^*(x,y)\\
  &=\nu(x)+C\max\{\mu(x)-\nu(y),0\}\sum_{y\in\Omega\wedge y\neq x}\max\{\nu(y)-\mu(y),0\}\\
  &=\nu(x)+C(\mu(x)-\nu(x))\sum_{y\in\bar{A}}(\nu(y)-\mu(y))
\end{align}
Note that $\sum_{y\in\bar{A}}(\nu(y)-\mu(y))=D_{TV}(\mu,\nu)$ by our definition of $\bar{A}$, so by setting $C=1/D_{TV}(\mu,\nu)$ we can get 
\begin{align}
  \forall x\in A,\quad \sum_{y\in\Omega}\omega^*(x,y)&=\nu(x)+\mu(x)-\nu(x)\\
  &=\mu(x)
\end{align}
We use the same $C$ and consider the cases where $x\in\bar{A}$. Similarly
\begin{align}
  \forall x\in \bar{A},\quad \sum_{y\in\Omega}\omega^*(x,y)&=\min\{\mu(x),\nu(x)\}+\sum_{y\in\Omega\wedge y\neq x}\omega^*(x,y)\\
  &=\mu(x)+D_{TV}(\mu,\nu)\max\{\mu(x)-\nu(x),0\}\sum_{y}\max\{\nu(y)-\mu(y),0\}\\
  &=\mu(x)+D_{TV}(\mu,\nu)\cdot 0\cdot\sum_{y}\max\{\nu(y)-\mu(y),0\}\\
  &=\mu(x)
\end{align}
Now we have proved such a coupling satisfies that the marginal distribution of $X$ is indeed $\mu(x)$. The same argument works for $Y$ as well. So by setting $C=1/D_{TV}(\mu,\nu)$ we actually constructed 
$$\omega^*(x,y)=\begin{cases}
  \min\{\mu(x),\nu(y)\},\quad x=y\\
  \frac{1}{D_{TV}(\mu,\nu)}\cdot\max\{\mu(x)-\nu(x),0\}\cdot\max\{\nu(y)-\mu(y),0\},\quad o.w.
  \end{cases}$$
Now we only need to show $Pr_{(X,Y)\sim\omega^*}(X\neq Y)$ is actually $D_{TV}(\mu,\nu)$.
\subsection{Proof of optimal coupling}
We show by calculating $Pr_{(X,Y)\sim\omega^*}(X=Y)$.\\
\begin{align}
  Pr_{(X,Y)\sim\omega^*}(X=Y)&=\sum_{x\in\Omega}\min\{\mu(x),\nu(x)\}\\
  &=\sum_{x\in A}\nu(x)+\sum_{x\in\bar{A}}\mu(x)\\
  &=\nu(A)+\mu(\bar{A})\\
  &=\nu(A)+(1-\mu(A))\\
  &=1-(\mu(A)-\nu(A))\\
  &=1-D_{TV}(\mu,\nu)
\end{align}
So $Pr_{(X,Y)\sim\omega^*}(X\neq Y)=D_{TV}(\mu,\nu)$
\newpage 


\section{Stochastic Dominance}
\textbf{\underline{The idea of proof comes from our teacher's lecture notes in previous year.}}\\
Following the idea of that lecture note, I will first prove the proposition about monotone coupling, because the other two questions are actually applications of this proposition.

\subsection{Monotone Coupling}
\textbf{Sufficiency:}\\
If a monotone coupling of $\mu$ and $\nu$ and exists, denoted by $\omega$. Then 
\begin{align}
  \forall a\in\Omega, Pr_{Y\sim\nu}(Y\geq a)&=Pr_{(X,Y)\sim\omega}(Y\geq a)\\
  &=Pr_{(X,Y)\sim\omega}(X\geq Y\wedge Y\geq a)+Pr_{(X,Y)\sim\omega}(X < Y\wedge Y\geq a)\\
  &=Pr_{(X,Y)\sim\omega}(X\geq Y\wedge Y\geq a)\\
  &\leq Pr_{(X,Y)\sim\omega}(X\geq a)\\
  &=Pr_{X\sim\mu}(X\geq a)
\end{align}
Hence $Pr_{X\sim\mu}(X\geq a)\geq Pr_{Y\sim\nu}(Y\geq a),\forall a\in\Omega$.\\
\textbf{Necessity:}\\
We prove by showing that there is a method of constructing such a monotone coupling. The basic idea is to construct this coupling greedily.\\
Firstly since $\Omega$ is a finite set of integers, there exists a smallest element $a_0$. By the stochastic dominance of $\mu$ over $\nu$, $\mu(a_0)<\nu(a_0)$. So in our coupling $\Pr(X=Y=a_0)$ should be $\mu(a_0)$. We can use $\{a_0,a_1,a_2,a_3\dots,a_n\}$ to denote $\Omega$. Then it follows that $\Pr(X=a_0,Y>a_0)=0$. So we have assigned valid values for all events in the coupling with $X=a_0$.\\
Now we consider the case with $X=a_1$. It is always possible to assign $\Pr(X=Y=a_1)=\min\{\mu(a_1),\nu(a_1)\}$. Then if $\min\{\mu(a_1),\nu(a_1)\}=\mu(a_1)$, we can assign $\Pr(X=a_1,Y\neq a_1)=0$ and we are done for cases with $X=a_1$. If $\min\{\mu(a_1),\nu(a_1)\}=\nu(a_1)$, Let $\Pr(X=a_1,Y=a_0)=\mu(a_1)-\nu(a_1)$, which is possible because $\Pr(X\leq a_1)<\Pr(Y\leq a_1)$. Assign other cases with $0$ and we are also done for $X=a_1$.\\
Then we can always carry out such a process step by step.\\
To be specific, for the case $X=a_k$, first assign $\Pr(X=a_k,Y>a_k)=0$ and $\Pr(X=a_k,Y=a_k)=\min\{\mu(a_k),\nu(a_k)\}$, if $\min\{\mu(a_k),\nu(a_k)\}=\mu(x_k)$ let $\Pr(X=a_k,Y\neq a_k)=0$ and we are done. If not, we assign values for $\Pr(X=a_k,Y=a_l),l<k$ by a descending order of $l$. First we maximize $\Pr(X=a_k,Y=a_{k-1}$), then we maximize $\Pr(X=a_k,Y=a_{k-2})$\dots We are done whenever $\sum_{j=0}^{n}\Pr(X=a_k,Y=a_{k-j})=\mu(a_k)$ for some $n$, This process is always possible by the stochastic dominance of $\mu$ over $\nu$ and by our construction process from the smallest $a_0$ to the largest $a_n$.
\subsection{Binomial Distribution}
\textbf{Sufficiency:}\\
If $p\geq q$, suppose $X\sim\text{Binom}(n,p)$, $Y\sim\text{Binom}(n,q)$. We define such a coupling $\omega$ of these two Binomial Distributions where we do $n$ trials and for these $n$ trials we independetly pick a real $r$ in $[0,1]$ uniformly at random and every trial is independent. Then let $X=x$ where $x$ is the number of these trials with $r\leq p$ and let $Y=y$ where $y$ is the number of these trials with $r\leq q$.\\
By our definition we can see clearly $Pr_{(X,Y)\sim\omega}(X\geq Y)=1$. So there exists a monotone coupling of $\text{Binom}(n,p)$ and $\text{Binom}(n,q)$. By the proposition we have proven above, $\text{Binom}(n,p)\succeq\text{Binom}(n,q)$.\\
\textbf{Necessity:}\\
Prove by contradiction. If $p<q$, consider the case where $a=n$. $Pr(X\geq a)=Pr(X=n)=p^n<q^n=Pr(Y=n)=Pr(Y\geq a)$. This violates $\text{Binom}(n,p)\succeq\text{Binom}(n,q)$. So $p\geq q$.
\subsection{Random Graph}
Suppose $G\sim\mathcal{G}(n,p)$, $H\sim\mathcal{G}(n,q)$. Consider such a coupling $\omega$ of $\mathcal{G}(n,p)$ and $\mathcal{G}(n,q)$ where we generate $G$ and $H$ simultaneously. For each pair of vertices $\{i,j\}$ we independently pick a real $r$ in $[0,1]$ uniformly at random. Let $G$ have edge $\{i,j\}$ iff $r\leq p$ and let $H$ have edge $\{i,j\}$ iff $r\leq q$.\\ 
For any $p,q\in[0,1]$ satisfying $p\geq q$, $H$ is a subgraph of $G$, so $\omega$ is a monotone coupling. So by the proposition we have proven $Pr_{G\sim\mathcal{G}(n,p)}(G\text{ is connected})\geq Pr_{H\sim\mathcal{G}(n,q)}(H\text{ is connected})$

\newpage
\section{Total Variation Distance is Non-Increasing}
Let $X_0\sim\mu_0$ and $Y_0\sim\pi$. For any $t\geq 0$, we can couple the distributions of random variables $X_t$ and $Y_t$ such that $Pr(X_t\neq Y_t)=\Delta(t)$. This coupling is feasible because we have proven, in problem 1, that an optimal coupling exists.\\
Then we can construct a coupling of the distributions of  $X_{t+1}$ and $Y_{t+1}$ with this coupling. We define 
$$\begin{cases}
  X_{t+1}=Y_{t+1},\quad \text{if} \quad X_t=Y_t\\
  X_{t+1}\sim\mu_{t+1}, Y_{t+1}\sim\pi \quad \text{if} \quad X_t\neq Y_t
\end{cases}$$
Then by the coupling lemma again we have 
\begin{align}
  \Delta(t+1)\leq Pr(X_{t+1}\neq Y_{t+1})
\end{align}
By our construction of coupling at $t$ and $t+1$ we have
\begin{align}
  &\quad\enspace  Pr(X_{t+1}\neq Y_{t+1})\\
  &=Pr(X_{t+1}\neq Y_{t+1}|X_t=Y_t)Pr(X_t=Y_t)+Pr(X_{t+1}\neq Y_{t+1}|X_t\neq Y_t)Pr(X_t\neq Y_t)\\
  &\leq Pr(X_t\neq Y_t)
\end{align} 
That inequality is true because all posibilities must be in $[0,1]$.\\
So it follows That
$$\Delta(t+1)\leq Pr(X_{t+1}\neq Y_{t+1})\leq Pr(X_t\neq Y_t)=\Delta(t),\forall t\geq 0$$
And that is true for any $t\geq 0$ by induction. The induction process is actually how the Markov Chain $X_t$ and $Y_t$ forms. Since any step of this induction process is the same as our construction of coupling and proof above, we do not write it again.

\newpage
\section{Acknowledgements}
\subsection{}
For Problem 1 the inspiration of constructing the optimal coupling is from the lecture notes of a MIT course 6.896 Probability and Computation.
Actually my construction is not entirely like its demonstration, but the core ideas are the same.\\
\subsection{}
For Problem 2 I actually learnt our teacher's lecture notes 5 from last year and then solved these questions. I think it will be extremely difficult to solve these questions without knowing the theorem about monotone coupling.
\subsection{}
For Problem 3 I referred to the lecture note 7 of a UC Berkeley course CS294 Partition Functions(do not know why such a course contains materials about our course). I cannot justify that my works are different from that lecture note because I cannot figure out another easy approach to solve this problem.
\end{document}
