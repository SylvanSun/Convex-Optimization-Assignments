\documentclass[12pt,letterpaper]{article}
\usepackage{fullpage}
\usepackage[top=2cm, bottom=4.5cm, left=2.5cm, right=2.5cm]{geometry}
\usepackage{amsmath,amsthm,amsfonts,amssymb,amscd}
\usepackage{lastpage}
\usepackage{enumerate}
\usepackage{fancyhdr}
\usepackage{mathrsfs}
\usepackage{xcolor}
\usepackage{graphicx}
\usepackage{listings}
\usepackage{hyperref}
\usepackage{mathtools}
\usepackage{xfrac}
\usepackage{bbm}

\hypersetup{
  colorlinks=true,
  linkcolor=blue,
  linkbordercolor={0 0 1}
}
\linespread{1.1}
 
\renewcommand\lstlistingname{Algorithm}
\renewcommand\lstlistlistingname{Algorithms}
\def\lstlistingautorefname{Alg.}


\lstdefinestyle{Python}{
    language        = Python,
    frame           = lines, 
    basicstyle      = \footnotesize,
    keywordstyle    = \color{blue},
    stringstyle     = \color{green},
    commentstyle    = \color{red}\ttfamily
}

\setlength{\parindent}{0.0in}
\setlength{\parskip}{0.05in}

\newcommand\course{Stochastic Processes}
\newcommand\hwnumber{5}
\newcommand\NetIDa{SUN Yilin}
\newcommand\NetIDb{520030910361}

\pagestyle{fancyplain}
\headheight 35pt
\lhead{\NetIDa}
\lhead{\NetIDa\\\NetIDb}
\chead{\textbf{\Large Homework \hwnumber}}
\rhead{\course \\ \today}
\lfoot{}
\cfoot{}
\rfoot{\small\thepage}
\headsep 1.5em

\begin{document}

\section{}
\subsection{}
Assume the first customer arrives at time $T-s+t$, $0 \leq t\leq s$.
Since customers arrive according to a Poisson process of rate $\lambda$ per hour,
the time it takes for the first customer to arrive follows Exponential distribution with parameter $\lambda$.
Then the time it takes for the second customer to arrive also follows a same distribution.
So the probability for achieving his goal is 
$$\int_{0}^{s}\lambda e^{-\lambda t}e^{-\lambda(s-t)}dt$$
which is 
$$\int_{0}^{s}\lambda e^{-\lambda s}dt$$
which is $\lambda se^{-\lambda s}$.
\subsection{}
To achieve the best probability, 
simply take the derivative and the result is 
$$(1-\lambda s)\lambda e^{-\lambda s}$$
So the optimal value of $s$ is $\frac{1}{\lambda}$ 
and the corresponding success probability is $\frac{1}{e}$.

\section{}
\subsection{}
We prove this by brutal calculations.\newline
To prove that $$\Pr(X=\lambda +k)\geq \Pr(X=\lambda -k-1)$$
We only need to show 
$$e^{-\lambda}\frac{\lambda^{\lambda+k}}{(\lambda +k)!}\geq e^{-\lambda}\frac{\lambda^{\lambda-k-1}}{(\lambda -k-1)!}$$
This entails
$$\lambda^{2k+1}\geq \frac{(\lambda +k)!}{(\lambda-k-1)!}$$
or equivalently 
$$\lambda^{2k+1}\geq (\lambda+k)(\lambda+k-1)\cdots (\lambda -k)$$
But it is obvious that $\lambda^2\geq \lambda^2 -a^2$ for any $a$.
So that inequality holds for any $k=0,1,2,\dots,\lambda-1$.
So for any $k=0,1,\dots,\lambda-1$,
it holds that $\Pr(X=\lambda +k)\geq \Pr(X=\lambda -k-1)$.
It then follows that $\Pr(X\geq\lambda)\geq \frac{1}{2}$.
\subsection{}
As shown in the lecture notes,
$$\mathbb{E}\left[f(Y_1,Y_2,\dots,Y_n)\right]=
\sum_{k=0}^{\infty}\mathbb{E}\left[f(Y_1,Y_2,\dots,Y_n)\bigg|
\sum_{i=1}^{n}Y_i=k\right]
\Pr(\sum_{i=1}^{n}Y_i=k)$$
In the lecture notes, to derive an inequality,
we discard all the other terms with only 
$\sum_{i=1}^{n}Y_i=m$ left.
This is too loose.
Here to prove this inequality we only discard 
the terms from $\sum_{i=1}^{n}Y_i=0$ to $\sum_{i=1}^{n}Y_i=m-1$,
this gives us 
$$\mathbb{E}\left[f(Y_1,Y_2,\dots,Y_n)\right]\geq
\sum_{k=m}^{\infty}\mathbb{E}\left[f(Y_1,Y_2,\dots,Y_n)\bigg|
\sum_{i=1}^{n}Y_i=k\right]
\Pr(\sum_{i=1}^{n}Y_i=k)$$ 
Since $\mathbb{E}\left[f(X_1,X_2,\dots,X_n)\right]$ is monotonically increasing in $m$,
$$\mathbb{E}\left[f(Y_1,Y_2,\dots,Y_n)\bigg|
\sum_{i=1}^{n}Y_i=k\right]\geq \mathbb{E}\left[f(X_1,X_2,\dots,X_n)\right], \forall k\geq m$$
Then the inequality becomes 
$$\mathbb{E}\left[f(Y_1,Y_2,\dots,Y_n)\right]\geq
\mathbb{E}\left[f(X_1,X_2,\dots,X_n)\right]
\sum_{k=m}^{\infty}\Pr(\sum_{i=1}^{n}Y_i=k)$$ 
Since $\sum_{i=1}^{n}Y_i\sim \text{Poisson}(m)$,
from problem 2.1 we know 
$$\sum_{k=m}^{\infty}\Pr(\sum_{i=1}^{n}Y_i=k)
=\Pr(\sum_{i=1}^{n}Y_i\geq m)\geq\frac{1}{2}$$
So $$\mathbb{E}\left[f(Y_1,Y_2,\dots,Y_n)\right]\geq
\mathbb{E}\left[f(X_1,X_2,\dots,X_n)\right]\frac{1}{2}$$
Or equivalently,
$$\mathbb{E}\left[f(X_1,X_2,\dots,X_n)\right]\leq 
2\cdot\mathbb{E}\left[f(Y_1,Y_2,\dots,Y_n)\right]$$ 
\subsection{}

\end{document}
