\documentclass[12pt,letterpaper]{article}
\usepackage{fullpage}
\usepackage[top=2cm, bottom=4.5cm, left=2.5cm, right=2.5cm]{geometry}
\usepackage{amsmath,amsthm,amsfonts,amssymb,amscd}
\usepackage{lastpage}
\usepackage{enumerate}
\usepackage{fancyhdr}
\usepackage{mathrsfs}
\usepackage{xcolor}
\usepackage{graphicx}
\usepackage{listings}
\usepackage{hyperref}
\usepackage{mathtools}
\usepackage{xfrac}
\usepackage{bbm}

\hypersetup{
  colorlinks=true,
  linkcolor=blue,
  linkbordercolor={0 0 1}
}
\linespread{1.1}
 
\renewcommand\lstlistingname{Algorithm}
\renewcommand\lstlistlistingname{Algorithms}
\def\lstlistingautorefname{Alg.}


\lstdefinestyle{Python}{
    language        = Python,
    frame           = lines, 
    basicstyle      = \footnotesize,
    keywordstyle    = \color{blue},
    stringstyle     = \color{green},
    commentstyle    = \color{red}\ttfamily
}

\setlength{\parindent}{0.0in}
\setlength{\parskip}{0.05in}

\newcommand\course{Stochastic Processes}
\newcommand\hwnumber{4}
\newcommand\NetIDa{SUN Yilin}
\newcommand\NetIDb{520030910361}

\pagestyle{fancyplain}
\headheight 35pt
\lhead{\NetIDa}
\lhead{\NetIDa\\\NetIDb}
\chead{\textbf{\Large Homework \hwnumber}}
\rhead{\course \\ \today}
\lfoot{}
\cfoot{}
\rfoot{\small\thepage}
\headsep 1.5em

\begin{document}

\section{Doob's Martingale Inequality}
Consider $\tau=\arg\min_{t\leq n}\{X_t\geq\alpha\}$ or $\tau=n$ if $\forall 0\leq t\leq n, X_t<\alpha$.\\
Clearly $\tau$ is a stopping time because by our definition 
for any $t\geq 0$, $\mathbbm{1}[\tau\leq t]$ is $\mathcal{F}_t$-measurable.\\
Then denote event $X_{\tau}\geq\alpha$ by $A$,
denote event $\max_{0\leq t\leq n}X_t\geq\alpha$ by $B$.
We have $B\subset A$ because by our definition of stopping time $\tau$,
if $\exists k, 0\leq k\leq n$ such that $X_t\geq\alpha$,
hence $\max_{0\leq t\leq n}X_t\geq\alpha$,
then it must follows that $X_{\tau}\geq\alpha$,
We know that if $B\subset A$, then $\Pr(B)\leq\Pr(A)$.
This means thats
$$\Pr \left[\max_{0\leq t\leq n}X_t\geq\alpha\right]\leq \Pr\left[X_{\tau}\geq\alpha\right]$$
Since $X_t\geq 0$, by applying the Markov Inequality we have
$$\Pr\left[X_{\tau}\geq\alpha\right]\leq \frac{\mathbb{E}\left[X_{\tau}\right]}{\alpha}$$
By our definition of $\tau$ we can easily see $\Pr[\tau\leq n]=1$,
which means that $\tau$ is bounded almost surely,
satisfying the first condition for Optional Stopping Theorem.
Then by OST we have
$$\mathbb{E}[X_{\tau}]=\mathbb{E}[{X_0}]$$
Adding up all the inequalities together we get 
$$\Pr \left[\max_{0\leq t\leq n}X_t\geq\alpha\right]\leq \frac{\mathbb{E}\left[X_{0}\right]}{\alpha}$$
which completes our proof.

\section{Biased One-dimensional Random Walk}
\subsection{}
\begin{align}
  \mathbb{E}(S_{t+1}|\overline{Z_{1,n}})
  &=\mathbb{E}(S_t+Z_{t+1}+2p-1|\overline{Z_{1,n}})\\
  &=S_t+2p-1+\mathbb{E}(Z_{t+1}|\overline{Z_{1,n}})\\
  &=S_t+2p-1+(-1)\cdot p+1\cdot(1-p)\\
  &=S_t
\end{align}
So $\{S_t\}$ is a martingale.
\subsection{}
\begin{align}
  \mathbb{E}(P_{t+1}|\overline{Z_{1,n}})
  &=\mathbb{E}((\frac{p}{1-p})^{X_t+Z_{t+1}}|\overline{Z_{1,n}})\\
  &=\mathbb{E}((\frac{p}{1-p})^{X_t}\cdot(\frac{p}{1-p})^{Z_{t+1}}|\overline{Z_{1,n}})\\
  &=P_t\cdot\mathbb{E}((\frac{p}{1-p})^{Z_{t+1}}|\overline{Z_{1,n}})\\
  &=P_t\cdot(\frac{p}{1-p}\cdot(1-p)+\frac{1-p}{p}\cdot p)\\
  &=P_t
\end{align}
So $\{P_t\}$ is a martingale.
\subsection{}
If $p=\frac{1}{2}$, we have shown in class that $\mathbb{E}(\tau)=ab$ in class.
Here we only show the case where $p\neq\frac{1}{2}$.\\
Clearly, $\Pr(\tau<\infty)$ also holds when $p\neq\frac{1}{2}$. 
$|X_t|$ is bounded, so $|P_t|$ is bounded,
indicating that $\{P_t\}$ satisfies the second condition of OST.
Also, $\mathbb{E}(|S_{t+1}-S_{t}||\mathcal{F}_t)\leq 2p+1$,
indicating that $\{S_t\}$ satisfies the third condition of OST.
So we can apply the Optional Stopping theorem on them and thus we have
$\mathbb{E}(S_{\tau})=\mathbb{E}(S_{1})$ and $\mathbb{E}(P_{\tau})=\mathbb{E}(P_{1})$.\\
Denote $\Pr$(ending at $-a$) by $P_a$, $\Pr$(ending at $b$) by $P_b$.
From $\mathbb{E}(S_{\tau})=\mathbb{E}(S_{1})=0$ we have 
$$\mathbb{E}(\tau)\cdot(2p-1)=aP_a-bP_b$$
From $\mathbb{E}(P_{\tau})=\mathbb{E}(P_{1})=1$ we have
$$\left(\frac{p}{1-p}\right)^{-a}P_a+\left(\frac{p}{1-p}\right)^{b}P_b=1$$
By the latter equation we can calculate $P_a$ and $P_b$,
$$P_a=\frac{1-\left(\frac{p}{1-p}\right)^b}{\left(\frac{p}{1-p}\right)^{-a}-\left(\frac{p}{1-p}\right)^b}\qquad
P_b=\frac{\left(\frac{p}{1-p}\right)^{-a}-1}{\left(\frac{p}{1-p}\right)^{-a}-\left(\frac{p}{1-p}\right)^b}$$
By putting these two results to the former equation we get
$$\mathbb{E}(\tau)=\frac{(a+b)-a\left(\frac{p}{1-p}\right)^b-b\left(\frac{p}{1-p}\right)^{-a}}
{(2p-1)\left[\left(\frac{p}{1-p}\right)^{-a}-\left(\frac{p}{1-p}\right)^b\right]} \quad (p\neq\frac{1}{2})$$

\section{Longest Common Subsequence}

\subsection{}
\textbf{Existence of $c_1$:}\\
We first find a $c_1$ for $n=2$ and then show that this $c_1$ works for general $n$.\\
With $n=2$, the string $x$, $y$ can only take the form of $00,01,10,11$.
It is very easy for us to calculate $\mathbb{E}(X)$ directly.
The result is $\mathbb{E}(X)=\frac{9}{8}$.\\
Then clearly there exists $\frac{1}{2}<c_1<\frac{9}{16}$ such that with $n=2$, $c_1n<\mathbb{E}(X)$.\\
For a general $n$, for convenience we first assume $n$ is even,
we can slways treat two consecutive elements together and use the result with $n=2$.
To be specific, with $x,y\in\{0,1\}^{n}$,
we can cut them into $x_1x_2,x_3x_4,x_5x_6,\dots,x_{n-1}x_n$ and $y_1y_2,y_3y_4,y_5y_6,\dots,y_{n-1}y_n$.
For any of those consecutive subsequences with length=$2$,
the expectated length of their longest common subsequence is larger than $2c_1$.
Then by linearity of expectations, $\mathbb{E}(X)>\frac{n}{2}\cdot 2c_1$,
or equivalently, $\mathbb{E}(X)>c_1n$ for the same $c_1$ as with case $n=2$.\\
For odd $n$, the analysis is the same with some trivial adjustment, say, 
putting the last three consecutive elements together and treat them individually.\\
\textbf{Existence of $c_2$:}\\
Following the hint, we would like to estimate the probability that 
two sequences with length $n$ has at least $c_2n$ common (but not necessarily consecutive) elements.\\
This probability is \textbf{smaller than}
$$\frac{\binom{n}{c_2n}}{2^{c_2n}}$$
where the numerator is picking such $c_2n$ elements to form a common subsequence
and the denominator is all possible situations for those $c_2n$ elements.\\
We want to show there exists a $c_2$ where such a probability approaches $0$ for sufficiently $n$.
The probability can be expressed as 
$$\frac{n!}{(c_2n)!(n-c_2n)!}\cdot\frac{1}{2^{c_2n}}$$
Then by applying the Stirling's Formula we get 
$$\frac{1}{\sqrt{2\pi nc_2(1-c_2)}}\left(\frac{1}{2^{c_2}c_{2}^{c_2}(1-c_2)^{1-c_2}}\right)^n$$
For a sufficiently larger $n$, 
if $c_2$ approaches $1$, clearly
$$\left(\frac{1}{2c_{2}^{c_2}(1-c_2)^{1-c_2}}\right)^n$$ 
approaches $0$ with exponential speed (approximately $(\frac{1}{2})^n$).\\
Also, $\sqrt{2\pi n c_2(1-c_2)}\to+\infty$ with such a constant $c_2<1$.\\
Since out probability that two sequences with length $n$ has at least $c_2n$ common (but not necessarily consecutive) elements
is smaller than such a probability,
there exists a $c_2<1$ such that for sufficiently larger $n$, 
$\Pr$[the length of longest common ubsequence is larger than $c_2n$]=0.
It then follows that the expectated length of the longest common subsequence is less than $c_2n$.
So there exists $c_2<1$ such that $\mathbb{E}(X)<c_2n$.
\subsection{}
We can treat each single bit of $x$ and $y$ as i.i.d. random variables uniformly chosen from $0$ or $1$,
Then the function $X$ is actually $X(x_1,x_2,\dots,x_n,y_1,y_2,\dots,y_n)$
where $X$ calculates the length of longest common subsequence of two random strings 
$x=x_1x_2\dots x_n$ and $y=y_1y_2\dots y_n$.\\
Clearly $X$ is a $2$-Lipschitz function because flipping any single bit of a sequence 
will make the lenght of longest common subsequence change for $2$ at most.\\
Then by McDiarmid's Inequality,
for such a function $X$ on $2n$ variables satisfying $2$-Lipschitz condition,
$$\Pr(|X-\mathbb{E}(X)|\geq t)\leq e^{-\frac{2t^2}{2n\cdot2^2}}=e^{-\frac{t^2}{4n}}$$
Thus $X$ is well-concentrated around $\mathbb{E}(X)$.  
\section{Collaborators and Acknowledgements}
For problem 3.1, the hint is too abstract for me.
As a result, only reading the hint itself did not provide me 
the intuition for solving this problem.\\
I then resorted to Liyuan MAO for help.
He taught me in detail the method of estimating such a probability
and why this method would work.
I cannot say that I fully understood this method,
but at least I could perform some basic analysis at last.
\end{document}
