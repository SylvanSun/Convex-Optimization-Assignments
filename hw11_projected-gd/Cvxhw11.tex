\documentclass[12pt,letterpaper]{article}
\usepackage{fullpage}
\usepackage[top=2cm, bottom=4.5cm, left=2.5cm, right=2.5cm]{geometry}
\usepackage{amsmath,amsthm,amsfonts,amssymb,amscd}
\usepackage{lastpage}
\usepackage{enumerate}
\usepackage{fancyhdr}
\usepackage{mathrsfs}
\usepackage{xcolor}
\usepackage{graphicx}
\usepackage{listings}
\usepackage{hyperref}
\usepackage{mathtools}
\usepackage{xfrac}
\usepackage{subfigure}

\hypersetup{%
  colorlinks=true,
  linkcolor=blue,
  linkbordercolor={0 0 1}
}
\linespread{1.1}
 
\renewcommand\lstlistingname{Algorithm}
\renewcommand\lstlistlistingname{Algorithms}
\def\lstlistingautorefname{Alg.}

\lstdefinestyle{Python}{
    language        = Python,
    frame           = lines, 
    basicstyle      = \footnotesize,
    keywordstyle    = \color{blue},
    stringstyle     = \color{green},
    commentstyle    = \color{red}\ttfamily
}

\setlength{\parindent}{0.0in}
\setlength{\parskip}{0.05in}

% Edit these as appropriate
\newcommand\course{ConvexOptimization}
\newcommand\hwnumber{11}                  % <-- homework number
\newcommand\NetIDa{SUN Yilin}           % <-- NetID of person #1
\newcommand\NetIDb{520030910361}           % <-- NetID of person #2 (Comment this line out for problem sets)

\pagestyle{fancyplain}
\headheight 35pt
\lhead{\NetIDa}
\lhead{\NetIDa\\\NetIDb}                 % <-- Comment this line out for problem sets (make sure you are person #1)
\chead{\textbf{\Large Homework \hwnumber}}
\rhead{\course \\ \today}
\lfoot{}
\cfoot{}
\rfoot{\small\thepage}
\headsep 1.5em

\begin{document}

\section{}
\begin{figure}[htbp]
\centering
\subfigure[trajectory with initial point (-1,0.5)]{
\label{tr1}
\includegraphics[width=0.45\textwidth]{lasso_traces_t1_ss0.1.pdf}}
\subfigure[gap with initial point (-1,0.5)]{
\label{gap1}
\includegraphics[width=0.45\textwidth]{lasso_gap_t1_ss0.1.pdf}}
\caption{trajectories and gaps for Problem 1}
\label{trandgap1a}
\end{figure}
I choose the same stepsize as slides, which is 0.1 and above are the trajectories and gaps.\\
The solution is $\boldsymbol{w}^*=(1,0)$, which is what we have seen in class and the number of iterations is 69. The optimal value is $4.5$.
\section{}
\subsection*{(a).}
The Lagrange function is 
$$\mathcal{L}=e^{x_1}+e^{2x_2}+e^{2x_3}+\lambda(x_1+x_2+x_3-1)$$
By setting the partial derivatives to zero we get 
$$\begin{cases}
\frac{\partial\mathcal{L}}{\partial x_1}=e^{x_1}+\lambda=0\\
\frac{\partial\mathcal{L}}{\partial x_2}=2e^{2x_2}+\lambda=0\\
\frac{\partial\mathcal{L}}{\partial x_3}=2e^{2x_3}+\lambda=0\\
\frac{\partial\mathcal{L}}{\partial\lambda}=x_1+x_2+x_3-1=0
\end{cases}$$
By solving the upper three equations we get
$$\begin{cases}
x_1=ln(-\lambda)\\
x_2=\frac{1}{2}ln(-\frac{\lambda}{2})\\
x_3=\frac{1}{2}ln(-\frac{\lambda}{2})\\
\end{cases}$$
Then by solving the last equation we calculate that 
$$\lambda^*=-\sqrt{2e}$$
Finally the optimal solution along with the multiplier is 
$$\begin{cases}
x_1^*=\frac{1}{2}(1+ln2)\\
x_2^*=\frac{1}{4}(1-ln2)\\
x_3^*=\frac{1}{4}(1-ln2)\\
\lambda^*=-\sqrt{2e}
\end{cases}$$
The optimal value is 
$$f^*=2\sqrt{2e}$$

\subsection*{(b)}
I choose stepsize=0.1 again and it takes me 52 iterations to get the result.\\
The optimal solution is $(x_1^*,x_2^*,x_3^*)=(0.8466, 0.0767, 0.0767)$ and the optimal value is $f^*=4.6633$, which is indeed the result we have calculated above using the Lanrange multiplier method.
\end{document}
