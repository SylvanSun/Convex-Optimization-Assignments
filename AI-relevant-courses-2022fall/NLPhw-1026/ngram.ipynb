{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7ace494b",
   "metadata": {},
   "source": [
    "# n元语言模型回退算法\n",
    "\n",
    "本次作业要求补全本笔记中的n元语言模型的采用Good-Turing折扣的Katz回退算法。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "171d9726",
   "metadata": {},
   "source": [
    "### 预处理\n",
    "\n",
    "首先创建一些预处理函数。\n",
    "\n",
    "引入必要的模块，定义些类型别名。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0097797b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import itertools\n",
    "\n",
    "from typing import List, Dict, Tuple\n",
    "\n",
    "Sentence = List[str]\n",
    "IntSentence = List[int]\n",
    "\n",
    "Corpus = List[Sentence]\n",
    "IntCorpus = List[IntSentence]\n",
    "\n",
    "Gram = Tuple[int]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69c02038",
   "metadata": {},
   "source": [
    "下面的函数用于将文本正则化并词元化。该函数会将所有英文文本转为小写，去除文本中所有的标点，简单起见将所有连续的数字用一个`N`代替，将形如`let's`的词组拆分为`let`和`'s`两个词。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cd05065c",
   "metadata": {},
   "outputs": [],
   "source": [
    "_splitor_pattern = re.compile(r\"[^a-zA-Z']+|(?=')\")\n",
    "_digit_pattern = re.compile(r\"\\d+\")\n",
    "def normaltokenize(corpus: List[str]) -> Corpus:\n",
    "    \"\"\"\n",
    "    Normalizes and tokenizes the sentences in `corpus`. Turns the letters into\n",
    "    lower case and removes all the non-alphadigit characters and splits the\n",
    "    sentence into words and added BOS and EOS marks.\n",
    "\n",
    "    Args:\n",
    "        corpus - list of str\n",
    "\n",
    "    Return:\n",
    "        list of list of str where each inner list of str represents the word\n",
    "          sequence in a sentence from the original sentence list\n",
    "    \"\"\"\n",
    "\n",
    "    tokeneds = [ [\"<s>\"]\n",
    "               + list(\n",
    "                   filter(lambda tkn: len(tkn)>0,\n",
    "                       _splitor_pattern.split(\n",
    "                           _digit_pattern.sub(\"N\", stc.lower()))))\n",
    "               + [\"</s>\"]\n",
    "                    for stc in corpus\n",
    "               ]\n",
    "    return tokeneds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae2768c1",
   "metadata": {},
   "source": [
    "接下来定义两个函数用来从训练语料中构建词表，并将句子中的单词从字符串表示转为整数索引表示。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c4685897",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_vocabulary(corpus: Corpus) -> Dict[str, int]:\n",
    "    \"\"\"\n",
    "    Extracts the vocabulary from `corpus` and returns it as a mapping from the\n",
    "    word to index. The words will be sorted by the codepoint value.\n",
    "\n",
    "    Args:\n",
    "        corpus - list of list of str\n",
    "\n",
    "    Return:\n",
    "        dict like {str: int}\n",
    "    \"\"\"\n",
    "\n",
    "    vocabulary = set(itertools.chain.from_iterable(corpus))\n",
    "    vocabulary = dict(\n",
    "            map(lambda itm: (itm[1], itm[0]),\n",
    "                enumerate(\n",
    "                    sorted(vocabulary))))\n",
    "    return vocabulary\n",
    "\n",
    "def words_to_indices(vocabulary: Dict[str, int], sentence: Sentence) -> IntSentence:\n",
    "    \"\"\"\n",
    "    Convert sentence in words to sentence in word indices.\n",
    "\n",
    "    Args:\n",
    "        vocabulary - dict like {str: int}\n",
    "        sentence - list of str\n",
    "\n",
    "    Return:\n",
    "        list of int\n",
    "    \"\"\"\n",
    "\n",
    "    return list(map(lambda tkn: vocabulary.get(tkn, len(vocabulary)), sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "265aba2b",
   "metadata": {},
   "source": [
    "接下来读入训练数据，将数据预处理。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "19af69f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded training set.\n",
      "Preprocessed training set.\n"
     ]
    }
   ],
   "source": [
    "import functools\n",
    "\n",
    "with open(\"data/news.2007.en.shuffled.deduped.train\") as f:\n",
    "    texts = list(map(lambda l: l.strip(), f.readlines()))\n",
    "\n",
    "print(\"Loaded training set.\")\n",
    "\n",
    "corpus = normaltokenize(texts)\n",
    "vocabulary = extract_vocabulary(corpus)\n",
    "corpus = list(\n",
    "        map(functools.partial(words_to_indices, vocabulary),\n",
    "            corpus))\n",
    "\n",
    "print(\"Preprocessed training set.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "20a3c7d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3581, 165570, 178819, 62968, 165570, 58547, 180493, 41355, 82173, 135885, 51060, 166148, 179627, 169773, 36804, 165541, 83663, 106975, 76095, 118831, 63602, 135877, 41396, 120257, 55961, 37434, 135208, 165710, 79221, 81152, 5006, 3580]\n"
     ]
    }
   ],
   "source": [
    "print(corpus[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a007b899",
   "metadata": {},
   "source": [
    "### 设计模型\n",
    "\n",
    "参照公式\n",
    "\n",
    "$$\n",
    "P_{\\text{bo}}(w_k | W_{k-n+1}^{k-1}) = \\begin{cases}\n",
    "    d(W_{k-n+1}^k) \\dfrac{C(W_{k-n+1}^k)}{C(W_{k-n+1}^{k-1})} &  C(W_{k-n+1}^k) > 0 \\\\\n",
    "    \\alpha(W_{k-n+1}^{k-1}) P_{\\text{bo}}(w_k | W_{k-n+2}^{k-1}) &  \\text{否则} \\\\\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "实现n元语言模型及采用Good-Turing折扣的Katz回退算法。\n",
    "\n",
    "需要实现的功能包括：\n",
    "\n",
    "1. 统计各词组（gram）在训练语料中的频数\n",
    "2. 计算同频词组个数$N_r$\n",
    "3. 计算$d(W_{k-n+1}^k)$\n",
    "4. 计算$\\alpha(W_{k-n+1}^{k-1})$\n",
    "5. 根据公式计算回退概率\n",
    "6. 计算概率对数与困惑度（PPL）\n",
    "\n",
    "$d$与$\\alpha$如何计算可以参考作业文件中的算法说明以及[SRILM](http://www.speech.sri.com/projects/srilm/)的[`ngram-discount(7)`手册页](http://www.speech.sri.com/projects/srilm/manpages/ngram-discount.7.html)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fea6708a",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "class NGramModel:\n",
    "    def __init__(self, vocab_size: int, n: int = 4):\n",
    "        \"\"\"\n",
    "        Constructs `n`-gram model with a `vocab_size`-size vocabulnextry.\n",
    "\n",
    "        Args:\n",
    "            vocab_size - int\n",
    "            n - int\n",
    "        \"\"\"\n",
    "\n",
    "        self.vocab_size: int = vocab_size\n",
    "        self.n: int = n\n",
    "\n",
    "        self.frequencies: List[Dict[Gram, int]]\\\n",
    "            = [{} for _ in range(n)]\n",
    "        self.disfrequencies: List[Dict[Gram, int]]\\\n",
    "            = [{} for _ in range(n)]\n",
    "        \"\"\"\n",
    "         We use a list of length n to store the count of counts.\n",
    "         The keys of each dictionary are the frequencies of each grams \n",
    "         and the values are the count of all grams having the same frequency.\n",
    "        \"\"\"\n",
    "        \n",
    "        self.ncounts: List[Dict[int,int]]\\\n",
    "            = [{} for _ in range(n)]\n",
    "\n",
    "        self.discount_threshold:int = 7\n",
    "        self._d: Dict[Gram, float] = {} # we only need to return a float instead of a tuple\n",
    "        self._alpha: List[Dict[Gram, float]]\\\n",
    "            = [{} for _ in range(n)]\n",
    "\n",
    "        self.eps = 1e-10\n",
    "\n",
    "    def learn(self, corpus: IntCorpus):\n",
    "        \"\"\"\n",
    "        Learns the parameters of the n-gram model.\n",
    "\n",
    "        Args:\n",
    "            corpus - list of list of int\n",
    "        \"\"\"\n",
    "        for stc in corpus:\n",
    "            for i in range(1, len(stc)+1):\n",
    "                for j in range(min(i, self.n)):\n",
    "                    # TODO: count the frequencies of the grams\n",
    "                    \n",
    "                    gram = tuple(stc[i - j - 1: i])\n",
    "                    if gram not in self.frequencies[j].keys():\n",
    "                        self.frequencies[j][gram] = 1\n",
    "                    else:\n",
    "                        self.frequencies[j][gram] += 1\n",
    "        for i in range(1, self.n):\n",
    "            # TODO: calculates the value of $N_r$\n",
    "            \n",
    "            gram_sorted = sorted(self.frequencies[i].items(), key = lambda itm: itm[1])\n",
    "            gram_grouped = itertools.groupby(gram_sorted, key = lambda itm: itm[1])\n",
    "            \n",
    "            for freq, group in gram_grouped:\n",
    "                count = len(list(group))\n",
    "                self.ncounts[i][freq] = count\n",
    "\n",
    "    def d(self, gram: Gram) -> float:\n",
    "        \"\"\"\n",
    "        Calculates the interpolation coefficient.\n",
    "\n",
    "        Args:\n",
    "            gram - tuple of int\n",
    "\n",
    "        Return:\n",
    "            float\n",
    "        \"\"\"\n",
    "\n",
    "        if gram not in self._d:\n",
    "            # TODO: calculates the value of $d'$\n",
    "\n",
    "            theta = self.discount_threshold\n",
    "            idx = len(gram)-1\n",
    "            freq = self.frequencies[idx][gram]\n",
    "            \n",
    "            if freq > theta:\n",
    "                self._d[gram] = 1\n",
    "            else:\n",
    "                lmbda = self.ncounts[idx][1] / (self.ncounts[idx][1] - (theta + 1) * self.ncounts[idx][theta + 1])\n",
    "                self._d[gram] = lmbda * (freq + 1) * self.ncounts[idx][freq + 1] / (freq * self.ncounts[idx][freq]) + 1 - lmbda\n",
    "            \n",
    "        return self._d[gram]\n",
    "\n",
    "    def alpha(self, gram: Gram) -> float:\n",
    "        \"\"\"\n",
    "        Calculates the back-off weight alpha(`gram`)\n",
    "\n",
    "        Args:\n",
    "            gram - tuple of int\n",
    "\n",
    "        Return:\n",
    "            float\n",
    "        \"\"\"\n",
    "\n",
    "        n = len(gram)\n",
    "        if gram not in self._alpha[n]:\n",
    "            if gram in self.frequencies[n-1]:\n",
    "                # TODO: calculates the value of $\\alpha$\n",
    "                \n",
    "                numerator = 1\n",
    "                denominator = 1\n",
    "                for keys in self.frequencies[n]:\n",
    "                    if list(keys)[:-1] == list(gram):\n",
    "                        numerator -= self.__getitem__(keys)\n",
    "                        denominator -= self.__getitem__(tuple(list(keys)[1:]))\n",
    "                self._alpha[n][gram] = numerator/denominator\n",
    "            else:\n",
    "                self._alpha[n][gram] = 1.\n",
    "        return self._alpha[n][gram]\n",
    "\n",
    "    def __getitem__(self, gram: Gram) -> float:\n",
    "        \"\"\"\n",
    "        Calculates smoothed conditional probability P(`gram[-1]`|`gram[:-1]`).\n",
    "\n",
    "        Args:\n",
    "            gram - tuple of int\n",
    "\n",
    "        Return:\n",
    "            float\n",
    "        \"\"\"\n",
    "\n",
    "        n = len(gram)-1\n",
    "        if gram not in self.disfrequencies[n]:\n",
    "            if n>0:\n",
    "                # TODO: calculates the smoothed probability value according to the formulae\n",
    "                \n",
    "                gram1 = tuple(list(gram)[:-1])\n",
    "                gram2 = tuple(list(gram)[1:])\n",
    "                if gram in self.frequencies[n]:\n",
    "                    self.disfrequencies[n][gram] = self.d(gram) * self.frequencies[n][gram] / self.frequencies[n-1][gram1]\n",
    "                else:\n",
    "                    alpha = self.alpha(gram1) \n",
    "                    self.disfrequencies[n][gram] = alpha * self.__getitem__(gram2)\n",
    "                \n",
    "            else: # uni-gram, n == 0\n",
    "                self.disfrequencies[n][gram] = \\\n",
    "                self.frequencies[n].get(gram, self.eps)/float(len(self.frequencies[0]))\n",
    "        return self.disfrequencies[n][gram]\n",
    "\n",
    "    def log_prob(self, sentence: IntSentence) -> float:\n",
    "        \"\"\"\n",
    "        Calculates the log probability of the given sentence. Assumes that the\n",
    "        first token is always \"<s>\".\n",
    "\n",
    "        Args:\n",
    "            sentence: list of int\n",
    "\n",
    "        Return:\n",
    "            float\n",
    "        \"\"\"\n",
    "\n",
    "        log_prob = 0.\n",
    "        for i in range(4, len(sentence) + 1):\n",
    "            # TODO: calculates the log probability\n",
    "            log_prob += math.log(math.fabs(self.__getitem__(tuple(sentence[i-4: i]))))\n",
    "        log_prob += math.log(math.fabs(self.__getitem__(tuple(sentence[0: 2]))))\n",
    "        log_prob += math.log(math.fabs(self.__getitem__(tuple(sentence[0: 3]))))\n",
    "        \n",
    "        return log_prob\n",
    "\n",
    "    def ppl(self, sentence: IntSentence) -> float:\n",
    "        \"\"\"\n",
    "        Calculates the PPL of the given sentence. Assumes that the first token\n",
    "        is always \"<s>\".\n",
    "\n",
    "        Args:\n",
    "            sentence: list of int\n",
    "\n",
    "        Return:\n",
    "            float\n",
    "        \"\"\"\n",
    "\n",
    "        # TODO: calculates the PPL\n",
    "        return math.exp(-1 * self.log_prob(sentence) / (len(sentence) - 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bbcf678",
   "metadata": {},
   "source": [
    "### 训练与测试\n",
    "\n",
    "现在数据与模型均已齐备，可以训练并测试了。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd706656",
   "metadata": {},
   "source": [
    "训练模型："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "db5f988b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dumped model.\n"
     ]
    }
   ],
   "source": [
    "import pickle as pkl\n",
    "\n",
    "model = NGramModel(len(vocabulary))\n",
    "model.learn(corpus)\n",
    "with open(\"model.pkl\", \"wb\") as f:\n",
    "    pkl.dump(vocabulary, f)\n",
    "    pkl.dump(model, f)\n",
    "\n",
    "print(\"Dumped model.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5f97ca6",
   "metadata": {},
   "source": [
    "在测试集上测试计算困惑度："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eaf8b1a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model.\n",
      "11596.24408205463\n",
      "649.0700393897549\n",
      "280.50652589330446\n",
      "86.99104796351482\n",
      "8093.891527334422\n",
      "49.54787416952054\n",
      "18.697741170563283\n",
      "167.54124422647462\n",
      "572.5963752038066\n",
      "70.0843271671685\n",
      "268.20323670992417\n",
      "119.57822907373735\n",
      "3829.5883654747277\n",
      "1126.7856237844871\n",
      "459.5123919497324\n",
      "323.6437000817539\n",
      "9400.992473039732\n",
      "19207.38012768233\n",
      "124.68527253031358\n",
      "179.79219684423038\n",
      "757.250845763894\n",
      "1269.5367940774618\n",
      "358.4935979530156\n",
      "287.310618983294\n",
      "83.75252049939425\n",
      "91.21891877389903\n",
      "466.94679373129065\n",
      "10.30420213772118\n",
      "180.5612333870199\n",
      "225.75050184256676\n",
      "242.50347487754607\n",
      "411.5681379682549\n",
      "95.12489308019748\n",
      "160.0925457005122\n",
      "773.0158982016502\n",
      "76.53760195461322\n",
      "699.9098819813352\n",
      "515.4219624073479\n",
      "256.80114168970215\n",
      "4151.707901461987\n",
      "152.98326609233212\n",
      "909.4010942525434\n",
      "115.6130377168016\n",
      "106.07522374053934\n",
      "1542.3328894427275\n",
      "1284.1528982231564\n",
      "346.5842553372241\n",
      "704.1080576007696\n",
      "129.04279015303302\n",
      "364.39241823196176\n",
      "Avg:  1467.876555980158\n"
     ]
    }
   ],
   "source": [
    "with open(\"model.pkl\", \"rb\") as f:\n",
    "    vocabulary = pkl.load(f)\n",
    "    model = pkl.load(f)\n",
    "print(\"Loaded model.\")\n",
    "\n",
    "with open(\"data/news.2007.en.shuffled.deduped.test\") as f:\n",
    "    test_set = list(map(lambda l: l.strip(), f.readlines()))\n",
    "test_corpus = normaltokenize(test_set)\n",
    "test_corpus = list(\n",
    "        map(functools.partial(words_to_indices, vocabulary),\n",
    "            test_corpus))\n",
    "ppls = []\n",
    "for t in test_corpus:\n",
    "    ppls.append(model.ppl(t))\n",
    "    print(ppls[-1])\n",
    "print(\"Avg: \", sum(ppls)/len(ppls))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
